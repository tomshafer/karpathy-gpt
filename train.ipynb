{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data set\n",
    "with open(\"shakespeare.txt\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Input length = {len(text):,d} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:250])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary (NB: We aren't covering all ASCII)\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars), f\"(N={vocab_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s: str) -> int:\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "def decode(i: int) -> str:\n",
    "    return \"\".join([itos[n] for n in i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode(\"Hello there\"))\n",
    "print(decode(encode(\"Hello there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into PyTorch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets\n",
    "nsplit = int(0.9 * len(data))\n",
    "train, test = data[:nsplit], data[nsplit:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train[: block_size + 1], train[1 : block_size + 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of context sizes\n",
    "x = train[:block_size]\n",
    "y = train[1 : 1 + block_size]\n",
    "\n",
    "for t in range(block_size):\n",
    "    print(f\"target = {y[t]:>2d}, input = {x[:t+1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch\n",
    "def batch(t: torch.Tensor, batch_size: int, block_size: int) -> torch.Tensor:\n",
    "    ix = torch.randint(len(t) - block_size, (batch_size,))\n",
    "    x = torch.stack([t[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([t[i + 1 : i + 1 + block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "xb, yb = batch(train, 4, 8)\n",
    "\n",
    "print(f\"Inputs ({xb.shape}):\")\n",
    "print(xb, \"\\n\")\n",
    "\n",
    "print(f\"Targets ({yb.shape}):\")\n",
    "print(yb, \"\\n\")\n",
    "\n",
    "for b, t in it.product(range(4), range(8)):\n",
    "    print(f\"target = {yb[b,t]:>2d}, input = {xb[b,:t+1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram language model\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, y: Optional[Tensor] = None\n",
    "    ) -> tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Make a prediction of the next token given the current.\"\"\"\n",
    "        logits = self.embeddings(x)  # shape: (batch, time, vocab)\n",
    "        if y is None:\n",
    "            return logits, None\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.view(b * t, c)\n",
    "        loss = F.cross_entropy(logits, y.view(b * t))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x: Tensor, max: int = 1) -> Tensor:\n",
    "        \"\"\"Take an input (B, T) and sample a new token.\"\"\"\n",
    "        for _ in range(max):\n",
    "            # We feed the whole context for generality, though\n",
    "            # the BigramLM only uses the final token.\n",
    "            logits, _ = self(x)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            newx = torch.multinomial(probs, 1)\n",
    "            x = torch.cat((x, newx), -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "m = BigramLM(vocab_size)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "\n",
    "# Example: Starting from newline, generate 100 tokens\n",
    "print(logits.shape, loss)\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class timer:\n",
    "    def __enter__(self):\n",
    "        self.time = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        now = time.perf_counter()\n",
    "        print(f\"Timer = {now-self.time} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the bigram model now\n",
    "# Big LR because a small model\n",
    "m = BigramLM(vocab_size)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "with timer():\n",
    "    for step in range(10_000):\n",
    "        xb, yb = batch(train, batch_size=batch_size, block_size=8)\n",
    "        _, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += [loss.item()]\n",
    "    print(f\"Loss = {losses[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c5c8698cca2150aad024466f3880c2d0d5a446cc91adc7335b906b4b40b9d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
